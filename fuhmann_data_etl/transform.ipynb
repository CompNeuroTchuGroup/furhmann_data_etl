{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36msourcecode.Text.generate\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SQLContext\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.nio.file.attribute.BasicFileAttributes\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.nio.file._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.mutable.ArrayBuffer\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sourcecode.Text.generate\n",
    "import $ivy.`org.apache.spark::spark-sql:3.5.2`\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import java.nio.file.attribute.BasicFileAttributes\n",
    "import java.nio.file._\n",
    "import scala.collection.mutable.ArrayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@7f574f5a\n",
       "\u001b[36msqlContext\u001b[39m: \u001b[32mSQLContext\u001b[39m = org.apache.spark.sql.SQLContext@6481acf1\n",
       "\u001b[32mimport \u001b[39m\u001b[36msqlContext.implicits._\u001b[39m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.SparkConf\n",
    "val spark = SparkSession.builder.master(\"spark://spark:7077\").config(\"spark.executor.memory\", \"10g\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "val sqlContext = spark.sqlContext\n",
    "import sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msearch_files\u001b[39m"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search_files(path: String, extension: String): ArrayBuffer[Path] = {\n",
    "  val files = ArrayBuffer.empty[Path]\n",
    "  val root = Paths.get(path)\n",
    "\n",
    "  Files.walkFileTree(root, new SimpleFileVisitor[Path] {\n",
    "    override def visitFile(file: Path, attrs: BasicFileAttributes) = {\n",
    "      if (file.getFileName.toString.endsWith(extension)) {\n",
    "        files += file\n",
    "      }\n",
    "      FileVisitResult.CONTINUE\n",
    "    }\n",
    "  })\n",
    "\n",
    "  files\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-----------------+---+-------------------+----+-----------+-----+----------+---------------+-------------------+--------------------+--------------------+--------------------+-----+---------------------+-----------+-------+--------------------+--------------------+-----------------+------------+\n",
      "|ResoTrigger|               Time|         Position|Lap|           Velocity|Pump|optotrigger|Licks|Licking_MM|Position_binned|Position_binned_2cm|1_cm_binned_position|2_cm_binned_position|5_cm_binned_position|Lap_2|Stimulation condition|Stimulation|Zone ID|           file_name|         neural_data|__index_level_0__|Time_counter|\n",
      "+-----------+-------------------+-----------------+---+-------------------+----+-----------+-----+----------+---------------+-------------------+--------------------+--------------------+--------------------+-----+---------------------+-----------+-------+--------------------+--------------------+-----------------+------------+\n",
      "|          1|                0.0|211.1310592459605|1.0|-0.1730679442129551| 0.0|        1.0|  0.0|       0.0| [211.0, 212.0)|     [210.0, 212.0)|                 212|                 106|                  43|    1|                    0|      false|      4|ID18158/Day5/Data...|[-0.0453613288700...|                0|        NULL|\n",
      "|          2|0.03124183155095408|211.1310592459605|1.0| -0.404976346058852| 0.0|        1.0|  0.0|       0.0| [211.0, 212.0)|     [210.0, 212.0)|                 212|                 106|                  43|    1|                    0|      false|      4|ID18158/Day5/Data...|[-0.0064532286487...|                1|        NULL|\n",
      "|          3|0.06248366310190815|211.1310592459605|1.0|-0.5747038750782638| 0.0|        1.0|  0.0|       0.0| [211.0, 212.0)|     [210.0, 212.0)|                 212|                 106|                  43|    1|                    0|      false|      4|ID18158/Day5/Data...|[-0.0010424358770...|                2|        NULL|\n",
      "|          4|0.09372549465286223|211.1310592459605|1.0|-0.6913096533078872| 0.0|        1.0|  0.0|       0.0| [211.0, 212.0)|     [210.0, 212.0)|                 212|                 106|                  43|    1|                    0|      false|      4|ID18158/Day5/Data...|[-0.0379230715334...|                3|        NULL|\n",
      "|          5| 0.1249673262038163|211.1310592459605|1.0|-0.7633097032637437| 0.0|        1.0|  0.0|       0.0| [211.0, 212.0)|     [210.0, 212.0)|                 212|                 106|                  43|    1|                    0|      false|      4|ID18158/Day5/Data...|[-0.0092158457264...|                4|        NULL|\n",
      "+-----------+-------------------+-----------------+---+-------------------+----+-----------+-----+----------+---------------+-------------------+--------------------+--------------------+--------------------+-----+---------------------+-----------+-------+--------------------+--------------------+-----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_array\u001b[39m: \u001b[32mArrayBuffer\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m] = \u001b[33mArrayBuffer\u001b[39m(\n",
       "  [ResoTrigger: bigint, Time: double ... 19 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 19 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 19 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 19 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 19 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 19 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 19 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 17 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 18 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 19 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "...\n",
       "\u001b[36mdf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [ResoTrigger: bigint, Time: double ... 20 more fields]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load parquet files into a DataFrame array\n",
    "val df_array = search_files(\"/main/data/source_as_parquet\", \".parquet\")\n",
    "  .map { f =>\n",
    "    spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(f.toString)\n",
    "  }\n",
    "// Concat all\n",
    "val df = df_array.reduce((df1, df2) => df1.unionByName(df2, allowMissingColumns=true))\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+-----------------+----+--------------------+--------------------+--------------------+-------------------+------------------+-----+\n",
      "|              time|         position|original_velocity|pump|         neural_data|           file_name|                  dt|                 dx|          velocity|index|\n",
      "+------------------+-----------------+-----------------+----+--------------------+--------------------+--------------------+-------------------+------------------+-----+\n",
      "|0.9684967780795763|167.8834395309168|15.33076223119325| 0.0|[0.00292782419273...|ID17905/Day5/Data...|0.031241831550954058|0.11094945690160785|3.5513108993195277|    0|\n",
      "|0.9997386096305304|168.0412270034912|13.94933848866475| 0.0|[0.00290607741044...|ID17905/Day5/Data...|0.031241831550954058|0.15778747257439818| 5.050519279481222|    1|\n",
      "| 1.030980441181484|168.2425834116714|12.65062656640945| 0.0|[0.00314377625181...|ID17905/Day5/Data...|0.031241831550953614|0.20135640818020306| 6.445089746156606|    2|\n",
      "| 1.062222272732439|168.4732800797663|11.48072543172317| 0.0|[0.00356999124778...|ID17905/Day5/Data...|0.031241831550955057| 0.2306966680949074| 7.384223543957206|    3|\n",
      "| 1.093464104283393| 168.707300343927|10.46035441083599| 0.0|[0.00456834512351...|ID17905/Day5/Data...|0.031241831550953947|0.23402026416067656|   7.4906064255228|    4|\n",
      "+------------------+-----------------+-----------------+----+--------------------+--------------------+--------------------+-------------------+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">windowSpec</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">WindowSpec</span></span> = org.apache.spark.sql.expressions.WindowSpec@3e8a4580\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">dv</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">org</span></span>.<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">apache</span></span>.<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">spark</span></span>.<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">sql</span></span>.<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">package</span></span>.<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">DataFrame</span></span> = [time: double, position: double ... 8 more fields]</code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mwindowSpec\u001b[39m: \u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@3e8a4580\n",
       "\u001b[36mdv\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [time: double, position: double ... 8 more fields]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val windowSpec = Window.partitionBy(\"file_name\").orderBy(\"time\")\n",
    "var dv = df.select(\n",
    "      $\"Time\".as(\"time\"),\n",
    "      $\"Position\".as(\"position\"),\n",
    "      $\"Velocity\".as(\"original_velocity\"),\n",
    "      $\"Pump\".as(\"pump\"),\n",
    "      $\"neural_data\",\n",
    "      $\"file_name\"\n",
    ")\n",
    ".withColumn(\"dt\", \n",
    "  $\"time\" - when(\n",
    "    (lag(\"time\", 1).over(windowSpec)).isNull, \n",
    "    0\n",
    "  ).otherwise(\n",
    "    lag(\"time\", 1).over(windowSpec)\n",
    "  )\n",
    ")\n",
    ".withColumn(\"dx\",\n",
    "  $\"position\" - when(\n",
    "    (lag(\"position\", 1).over(windowSpec)).isNull, \n",
    "    0\n",
    "  ).otherwise(\n",
    "    lag(\"position\", 1).over(windowSpec)\n",
    "  )\n",
    ")\n",
    ".withColumn(\"velocity\",\n",
    "  when(\n",
    "    $\"dt\" === 0, \n",
    "    0\n",
    "  ).otherwise(\n",
    "    $\"dx\" / $\"dt\"\n",
    "  )\n",
    ")\n",
    ".withColumn(\"file_name\", regexp_replace(col(\"file_name\"), \"\\\\.[^.]+$\", \"\"))\n",
    ".na.drop()\n",
    ".where(!($\"pump\" < 1 && $\"pump\" > 0))\n",
    ".where(\"exists(neural_data, x -> x is not null)\")\n",
    ".withColumn(\"index\", monotonically_increasing_id())\n",
    "dv.createOrReplaceTempView(\"dv\")\n",
    "dv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmaxPosition\u001b[39m: \u001b[32mAny\u001b[39m = \u001b[32m359.964959290941\u001b[39m\n",
       "\u001b[36mpi\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m3.141592653589793\u001b[39m"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Positional encoding assuming that it starts on 0 and ends in maxPosition\n",
    "val maxPosition = df.select(max($\"position\")).head()(0)\n",
    "val pi = 3.141592653589793\n",
    "\n",
    "dv = dv.withColumn(\"positional_encoding\", sin($\"position\")*2.0*pi/maxPosition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv.write.partitionBy(\"file_name\").mode(\"overwrite\").parquet(\"/main/workspace/transformed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
