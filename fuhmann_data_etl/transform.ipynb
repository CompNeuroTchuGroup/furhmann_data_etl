{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36msourcecode.Text.generate\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SQLContext\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sourcecode.Text.generate\n",
    "import $ivy.`org.apache.spark::spark-sql:3.5.2`\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "24/12/09 16:00:36 INFO SparkContext: Running Spark version 3.5.2\n",
      "24/12/09 16:00:36 INFO SparkContext: OS info Linux, 6.8.0-49-generic, amd64\n",
      "24/12/09 16:00:36 INFO SparkContext: Java version 1.8.0_402\n",
      "24/12/09 16:00:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/09 16:00:36 INFO ResourceUtils: ==============================================================\n",
      "24/12/09 16:00:36 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/12/09 16:00:36 INFO ResourceUtils: ==============================================================\n",
      "24/12/09 16:00:36 INFO SparkContext: Submitted application: 26792421-d75c-4112-8b8c-9af0f064785d\n",
      "24/12/09 16:00:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/12/09 16:00:36 INFO ResourceProfile: Limiting resource is cpu\n",
      "24/12/09 16:00:36 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/12/09 16:00:36 INFO SecurityManager: Changing view acls to: jovyan\n",
      "24/12/09 16:00:36 INFO SecurityManager: Changing modify acls to: jovyan\n",
      "24/12/09 16:00:36 INFO SecurityManager: Changing view acls groups to: \n",
      "24/12/09 16:00:36 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/12/09 16:00:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: jovyan; groups with view permissions: EMPTY; users with modify permissions: jovyan; groups with modify permissions: EMPTY\n",
      "24/12/09 16:00:36 INFO Utils: Successfully started service 'sparkDriver' on port 39955.\n",
      "24/12/09 16:00:37 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/12/09 16:00:37 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/12/09 16:00:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/12/09 16:00:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/12/09 16:00:37 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/12/09 16:00:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1881337e-96fc-4d7c-a6c6-ffa206d7751f\n",
      "24/12/09 16:00:37 INFO MemoryStore: MemoryStore started with capacity 1909.8 MiB\n",
      "24/12/09 16:00:37 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/12/09 16:00:37 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/12/09 16:00:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/12/09 16:00:37 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark:7077...\n",
      "24/12/09 16:00:37 INFO TransportClientFactory: Successfully created connection to spark/172.21.0.3:7077 after 50 ms (0 ms spent in bootstraps)\n",
      "24/12/09 16:00:37 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20241209160037-0000\n",
      "24/12/09 16:00:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35673.\n",
      "24/12/09 16:00:37 INFO NettyBlockTransferService: Server created on 4b3ba8aa3939:35673\n",
      "24/12/09 16:00:37 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241209160037-0000/0 on worker-20241209155939-172.21.0.2-35577 (172.21.0.2:35577) with 10 core(s)\n",
      "24/12/09 16:00:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/12/09 16:00:37 INFO StandaloneSchedulerBackend: Granted executor ID app-20241209160037-0000/0 on hostPort 172.21.0.2:35577 with 10 core(s), 1024.0 MiB RAM\n",
      "24/12/09 16:00:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4b3ba8aa3939, 35673, None)\n",
      "24/12/09 16:00:37 INFO BlockManagerMasterEndpoint: Registering block manager 4b3ba8aa3939:35673 with 1909.8 MiB RAM, BlockManagerId(driver, 4b3ba8aa3939, 35673, None)\n",
      "24/12/09 16:00:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4b3ba8aa3939, 35673, None)\n",
      "24/12/09 16:00:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4b3ba8aa3939, 35673, None)\n",
      "24/12/09 16:00:38 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241209160037-0000/0 is now RUNNING\n",
      "24/12/09 16:00:38 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@3fdfce07\n",
       "\u001b[36msqlContext\u001b[39m: \u001b[32mSQLContext\u001b[39m = org.apache.spark.sql.SQLContext@30587267"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder.master(\"spark://spark:7077\").getOrCreate()\n",
    "val sqlContext = spark.sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 16:00:44 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/12/09 16:00:44 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.\n",
      "24/12/09 16:00:45 INFO InMemoryFileIndex: It took 30 ms to list leaf files for 1 paths.\n",
      "24/12/09 16:00:45 INFO SparkContext: Starting job: parquet at cmd3.sc:1\n",
      "24/12/09 16:00:45 INFO DAGScheduler: Got job 0 (parquet at cmd3.sc:1) with 1 output partitions\n",
      "24/12/09 16:00:45 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at cmd3.sc:1)\n",
      "24/12/09 16:00:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/12/09 16:00:45 INFO DAGScheduler: Missing parents: List()\n",
      "24/12/09 16:00:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at cmd3.sc:1), which has no missing parents\n",
      "24/12/09 16:00:45 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.9 KiB, free 1909.7 MiB)\n",
      "24/12/09 16:00:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.1 KiB, free 1909.7 MiB)\n",
      "24/12/09 16:00:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 4b3ba8aa3939:35673 (size: 37.1 KiB, free: 1909.8 MiB)\n",
      "24/12/09 16:00:45 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "24/12/09 16:00:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at cmd3.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/12/09 16:00:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "24/12/09 16:00:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.21.0.2, executor 0, partition 0, PROCESS_LOCAL, 9176 bytes) \n",
      "24/12/09 16:00:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.21.0.2:34085 (size: 37.1 KiB, free: 434.4 MiB)\n",
      "24/12/09 16:00:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 835 ms on 172.21.0.2 (executor 0) (1/1)\n",
      "24/12/09 16:00:46 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/12/09 16:00:46 INFO DAGScheduler: ResultStage 0 (parquet at cmd3.sc:1) finished in 1.000 s\n",
      "24/12/09 16:00:46 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/12/09 16:00:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/12/09 16:00:46 INFO DAGScheduler: Job 0 finished: parquet at cmd3.sc:1, took 1.059796 s\n",
      "24/12/09 16:00:47 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 4b3ba8aa3939:35673 in memory (size: 37.1 KiB, free: 1909.8 MiB)\n",
      "24/12/09 16:00:47 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.21.0.2:34085 in memory (size: 37.1 KiB, free: 434.4 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [ResoTrigger: bigint, Time: double ... 20 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df  = spark.read.parquet(\"/home/jovyan/parquet/ID18170/DataFrame_Imaging_spiking_18170_day9.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 16:01:02 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/12/09 16:01:02 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/12/09 16:01:02 INFO CodeGenerator: Code generated in 336.155587 ms\n",
      "24/12/09 16:01:02 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 361.3 KiB, free 1909.4 MiB)\n",
      "24/12/09 16:01:02 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 1909.4 MiB)\n",
      "24/12/09 16:01:02 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 4b3ba8aa3939:35673 (size: 35.7 KiB, free: 1909.8 MiB)\n",
      "24/12/09 16:01:02 INFO SparkContext: Created broadcast 1 from show at cmd4.sc:1\n",
      "24/12/09 16:01:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 41363938 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/12/09 16:01:03 INFO SparkContext: Starting job: show at cmd4.sc:1\n",
      "24/12/09 16:01:03 INFO DAGScheduler: Got job 1 (show at cmd4.sc:1) with 1 output partitions\n",
      "24/12/09 16:01:03 INFO DAGScheduler: Final stage: ResultStage 1 (show at cmd4.sc:1)\n",
      "24/12/09 16:01:03 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/12/09 16:01:03 INFO DAGScheduler: Missing parents: List()\n",
      "24/12/09 16:01:03 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at show at cmd4.sc:1), which has no missing parents\n",
      "24/12/09 16:01:03 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 29.9 KiB, free 1909.4 MiB)\n",
      "24/12/09 16:01:03 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 1909.4 MiB)\n",
      "24/12/09 16:01:03 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 4b3ba8aa3939:35673 (size: 8.9 KiB, free: 1909.8 MiB)\n",
      "24/12/09 16:01:03 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "24/12/09 16:01:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at cmd4.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/12/09 16:01:03 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "24/12/09 16:01:03 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.21.0.2, executor 0, partition 0, PROCESS_LOCAL, 9646 bytes) \n",
      "24/12/09 16:01:03 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.21.0.2:34085 (size: 8.9 KiB, free: 434.4 MiB)\n",
      "24/12/09 16:01:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.21.0.2:34085 (size: 35.7 KiB, free: 434.4 MiB)\n",
      "24/12/09 16:01:04 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 920 ms on 172.21.0.2 (executor 0) (1/1)\n",
      "24/12/09 16:01:04 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/12/09 16:01:04 INFO DAGScheduler: ResultStage 1 (show at cmd4.sc:1) finished in 0.980 s\n",
      "24/12/09 16:01:04 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/12/09 16:01:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/12/09 16:01:04 INFO DAGScheduler: Job 1 finished: show at cmd4.sc:1, took 0.994308 s\n",
      "24/12/09 16:01:04 INFO SparkContext: Starting job: show at cmd4.sc:1\n",
      "24/12/09 16:01:04 INFO DAGScheduler: Got job 2 (show at cmd4.sc:1) with 4 output partitions\n",
      "24/12/09 16:01:04 INFO DAGScheduler: Final stage: ResultStage 2 (show at cmd4.sc:1)\n",
      "24/12/09 16:01:04 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/12/09 16:01:04 INFO DAGScheduler: Missing parents: List()\n",
      "24/12/09 16:01:04 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at show at cmd4.sc:1), which has no missing parents\n",
      "24/12/09 16:01:04 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 29.9 KiB, free 1909.3 MiB)\n",
      "24/12/09 16:01:04 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 1909.3 MiB)\n",
      "24/12/09 16:01:04 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 4b3ba8aa3939:35673 (size: 8.9 KiB, free: 1909.7 MiB)\n",
      "24/12/09 16:01:04 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
      "24/12/09 16:01:04 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at show at cmd4.sc:1) (first 15 tasks are for partitions Vector(1, 2, 3, 4))\n",
      "24/12/09 16:01:04 INFO TaskSchedulerImpl: Adding task set 2.0 with 4 tasks resource profile 0\n",
      "24/12/09 16:01:04 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.21.0.2, executor 0, partition 1, PROCESS_LOCAL, 9646 bytes) \n",
      "24/12/09 16:01:04 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (172.21.0.2, executor 0, partition 2, PROCESS_LOCAL, 9646 bytes) \n",
      "24/12/09 16:01:04 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 4) (172.21.0.2, executor 0, partition 3, PROCESS_LOCAL, 9646 bytes) \n",
      "24/12/09 16:01:04 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 5) (172.21.0.2, executor 0, partition 4, PROCESS_LOCAL, 9646 bytes) \n",
      "24/12/09 16:01:04 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.21.0.2:34085 (size: 8.9 KiB, free: 434.3 MiB)\n",
      "24/12/09 16:01:04 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 115 ms on 172.21.0.2 (executor 0) (1/4)\n",
      "24/12/09 16:01:04 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 115 ms on 172.21.0.2 (executor 0) (2/4)\n",
      "24/12/09 16:01:04 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 4) in 124 ms on 172.21.0.2 (executor 0) (3/4)\n",
      "24/12/09 16:01:06 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 5) in 1822 ms on 172.21.0.2 (executor 0) (4/4)\n",
      "24/12/09 16:01:06 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/12/09 16:01:06 INFO DAGScheduler: ResultStage 2 (show at cmd4.sc:1) finished in 1.879 s\n",
      "24/12/09 16:01:06 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/12/09 16:01:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "24/12/09 16:01:06 INFO DAGScheduler: Job 2 finished: show at cmd4.sc:1, took 1.899144 s\n",
      "24/12/09 16:01:06 INFO CodeGenerator: Code generated in 28.094136 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-----------------+---+------------------+------------------+-----------+-----+--------------------+---------------+-------------------+--------------------+--------------------+--------------------+-----+---------------------+-----------+-------+------------+--------------------+--------------------+-----------------+\n",
      "|ResoTrigger|               Time|         Position|Lap|          Velocity|              Pump|optotrigger|Licks|          Licking_MM|Position_binned|Position_binned_2cm|1_cm_binned_position|2_cm_binned_position|5_cm_binned_position|Lap_2|Stimulation condition|Stimulation|Zone ID|Time_counter|           file_name|         neural_data|__index_level_0__|\n",
      "+-----------+-------------------+-----------------+---+------------------+------------------+-----------+-----+--------------------+---------------+-------------------+--------------------+--------------------+--------------------+-----+---------------------+-----------+-------+------------+--------------------+--------------------+-----------------+\n",
      "|          1|                0.0|286.8282968335617|1.0| 1.536018135201836|               1.0|          1|  0.0|                 1.0| [286.0, 287.0)|     [286.0, 288.0)|                 287|                 144|                  58|    1|                    0|      false|      6|           1|ID18170/DataFrame...|[NULL, NULL, NULL...|                0|\n",
      "|          2|0.03124183155095408|286.8710140933785|1.0| 1.477726941436118|               1.0|          1|  0.0|                 1.0| [286.0, 287.0)|     [286.0, 288.0)|                 287|                 144|                  58|    1|                    0|      false|      6|           1|ID18170/DataFrame...|[NULL, NULL, NULL...|                1|\n",
      "|          3|0.06248366310190815|286.8884339815762|1.0| 1.420189287878201|               1.0|          1|  0.0|                 1.0| [286.0, 287.0)|     [286.0, 288.0)|                 287|                 144|                  58|    1|                    0|      false|      6|           1|ID18170/DataFrame...|[NULL, NULL, NULL...|                2|\n",
      "|          4|0.09372549465286223|286.8884339815762|1.0| 1.340412648476529|               1.0|          1|  0.0|                 1.0| [286.0, 287.0)|     [286.0, 288.0)|                 287|                 144|                  58|    1|                    0|      false|      6|           1|ID18170/DataFrame...|[NULL, NULL, NULL...|                3|\n",
      "|          5| 0.1249673262038163|286.8884339815762|1.0| 1.241548397118296|               1.0|          1|  0.0|                 1.0| [286.0, 287.0)|     [286.0, 288.0)|                 287|                 144|                  58|    1|                    0|      false|      6|           1|ID18170/DataFrame...|[NULL, NULL, NULL...|                4|\n",
      "|          6| 0.1562091577547704|286.8884339815762|1.0| 1.130507063011102|0.3674121405750799|          1|  0.0|                 1.0| [286.0, 287.0)|     [286.0, 288.0)|                 287|                 144|                  58|    1|                    0|      false|      6|           1|ID18170/DataFrame...|[NULL, NULL, NULL...|                5|\n",
      "|          7| 0.1874509893057245|286.8884339815762|1.0| 1.013009492859946|               0.0|          1|  0.0|                 1.0| [286.0, 287.0)|     [286.0, 288.0)|                 287|                 144|                  58|    1|                    0|      false|      6|           1|ID18170/DataFrame...|[NULL, NULL, NULL...|                6|\n",
      "|          8| 0.2186928208566785|286.8884339815762|1.0|0.8936925125068033|               0.0|          1|  0.0| 0.05111821086261981| [286.0, 287.0)|     [286.0, 288.0)|                 287|                 144|                  58|    1|                    0|      false|      6|           1|ID18170/DataFrame...|[NULL, NULL, NULL...|                7|\n",
      "|          9| 0.2499346524076326|286.8884339815762|1.0|0.7762209586318712|               0.0|          1|  0.0|                 0.0| [286.0, 287.0)|     [286.0, 288.0)|                 287|                 144|                  58|    1|                    0|      false|      6|           1|ID18170/DataFrame...|[NULL, NULL, NULL...|                8|\n",
      "|         10| 0.2811764839585867|286.8884339815762|1.0|0.6634016121990145|               0.0|          1|  0.0|                 0.0| [286.0, 287.0)|     [286.0, 288.0)|                 287|                 144|                  58|    1|                    0|      false|      6|           1|ID18170/DataFrame...|[NULL, NULL, NULL...|                9|\n",
      "|         11| 0.3124183155095407|286.8884339815762|1.0|0.5572954979318747|               0.0|          1|  0.0|                 0.0| [286.0, 287.0)|     [286.0, 288.0)|                 287|                 144|                  58|    1|                    0|      false|      6|           1|ID18170/DataFrame...|[NULL, NULL, NULL...|               10|\n",
      "|         12| 0.3436601470604948|286.8884339815762|1.0|0.4593258379262454|               0.0|          1|  0.0|                 0.0| [286.0, 287.0)|     [286.0, 288.0)|                 287|                 144|                  58|    1|                    0|      false|      6|           1|ID18170/DataFrame...|[NULL, NULL, NULL...|               11|\n",
      "|         13| 0.3749019786114489|286.8884339815762|1.0|0.3705117405760814|               0.0|          1|  0.0|                 0.0| [286.0, 287.0)|     [286.0, 288.0)|                 287|                 144|                  58|    1|                    0|      false|      6|           1|ID18170/DataFrame...|[NULL, NULL, NULL...|               12|\n",
      "|         14|  0.406143810162403|286.8884339815762|1.0|0.2911403776474876|               0.0|          1|  0.0|                 0.0| [286.0, 287.0)|     [286.0, 288.0)|                 287|                 144|                  58|    1|                    0|      false|      6|           1|ID18170/DataFrame...|[NULL, NULL, NULL...|               13|\n",
      "|         15|  0.437385641713357|286.9012854765027|1.0|0.2237779252336207|               0.0|          1|  0.0|                 0.0| [286.0, 287.0)|     [286.0, 288.0)|                 287|                 144|                  58|    1|                    0|      false|      6|           1|ID18170/DataFrame...|[NULL, NULL, NULL...|               14|\n",
      "|         16| 0.4686274732643111|286.9266941573114|1.0| 0.186117284160695|               0.0|          1|  0.0|                 0.0| [286.0, 287.0)|     [286.0, 288.0)|                 287|                 144|                  58|    1|                    0|      false|      6|           1|ID18170/DataFrame...|[NULL, NULL, NULL...|               15|\n",
      "|         17| 0.4998693048152652|286.9650524360613|1.0|0.1782290819356278|               0.0|          1|  0.0|                 0.0| [286.0, 287.0)|     [286.0, 288.0)|                 287|                 144|                  58|    1|                    0|      false|      6|           1|ID18170/DataFrame...|[NULL, NULL, NULL...|               16|\n",
      "|         18| 0.5311111363662193|286.9982112550319|1.0|0.1992652703593539|               0.0|          1|  0.0|                 0.0| [286.0, 287.0)|     [286.0, 288.0)|                 287|                 144|                  58|    1|                    0|      false|      6|           1|ID18170/DataFrame...|[NULL, NULL, NULL...|               17|\n",
      "|         19| 0.5623529679171734|287.0117494710613|1.0|0.2331787459465936|               0.0|          1|  0.0|                 0.0| [287.0, 288.0)|     [286.0, 288.0)|                 288|                 144|                  58|    1|                    0|      false|      6|           1|ID18170/DataFrame...|[NULL, NULL, NULL...|               18|\n",
      "|         20| 0.5935947994681274|287.0419651995905|1.0|0.2698527146064112|               0.0|          1|  0.0|0.003194888178913738| [287.0, 288.0)|     [286.0, 288.0)|                 288|                 144|                  58|    1|                    0|      false|      6|           1|ID18170/DataFrame...|[NULL, NULL, NULL...|               19|\n",
      "+-----------+-------------------+-----------------+---+------------------+------------------+-----------+-----+--------------------+---------------+-------------------+--------------------+--------------------+--------------------+-----+---------------------+-----------+-------+------------+--------------------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
