{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36msourcecode.Text.generate\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SQLContext\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.nio.file.attribute.BasicFileAttributes\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.nio.file._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.mutable.ArrayBuffer\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sourcecode.Text.generate\n",
    "import $ivy.`org.apache.spark::spark-sql:3.5.2`\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import java.nio.file.attribute.BasicFileAttributes\n",
    "import java.nio.file._\n",
    "import scala.collection.mutable.ArrayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "24/12/13 12:39:05 INFO SparkContext: Running Spark version 3.5.2\n",
      "24/12/13 12:39:05 INFO SparkContext: OS info Linux, 6.8.0-50-generic, amd64\n",
      "24/12/13 12:39:05 INFO SparkContext: Java version 1.8.0_402\n",
      "24/12/13 12:39:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/13 12:39:05 INFO ResourceUtils: ==============================================================\n",
      "24/12/13 12:39:05 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/12/13 12:39:05 INFO ResourceUtils: ==============================================================\n",
      "24/12/13 12:39:05 INFO SparkContext: Submitted application: 428eb541-8d12-4714-ae6b-661adb9a0d61\n",
      "24/12/13 12:39:05 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 10240, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/12/13 12:39:05 INFO ResourceProfile: Limiting resource is cpu\n",
      "24/12/13 12:39:05 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/12/13 12:39:05 INFO SecurityManager: Changing view acls to: jovyan\n",
      "24/12/13 12:39:05 INFO SecurityManager: Changing modify acls to: jovyan\n",
      "24/12/13 12:39:05 INFO SecurityManager: Changing view acls groups to: \n",
      "24/12/13 12:39:05 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/12/13 12:39:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: jovyan; groups with view permissions: EMPTY; users with modify permissions: jovyan; groups with modify permissions: EMPTY\n",
      "24/12/13 12:39:05 INFO Utils: Successfully started service 'sparkDriver' on port 33723.\n",
      "24/12/13 12:39:05 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/12/13 12:39:05 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/12/13 12:39:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/12/13 12:39:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/12/13 12:39:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/12/13 12:39:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-75810270-e8dd-4d90-92fb-0fcd2c21106c\n",
      "24/12/13 12:39:05 INFO MemoryStore: MemoryStore started with capacity 15.8 GiB\n",
      "24/12/13 12:39:05 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/12/13 12:39:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/12/13 12:39:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/12/13 12:39:06 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark:7077...\n",
      "24/12/13 12:39:06 INFO TransportClientFactory: Successfully created connection to spark/172.19.0.4:7077 after 18 ms (0 ms spent in bootstraps)\n",
      "24/12/13 12:39:06 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20241213123906-0000\n",
      "24/12/13 12:39:06 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241213123906-0000/0 on worker-20241213123842-172.19.0.8-32993 (172.19.0.8:32993) with 10 core(s)\n",
      "24/12/13 12:39:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40289.\n",
      "24/12/13 12:39:06 INFO NettyBlockTransferService: Server created on 6cad2e46d2a5:40289\n",
      "24/12/13 12:39:06 INFO StandaloneSchedulerBackend: Granted executor ID app-20241213123906-0000/0 on hostPort 172.19.0.8:32993 with 10 core(s), 10.0 GiB RAM\n",
      "24/12/13 12:39:06 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241213123906-0000/1 on worker-20241213123842-172.19.0.7-44865 (172.19.0.7:44865) with 10 core(s)\n",
      "24/12/13 12:39:06 INFO StandaloneSchedulerBackend: Granted executor ID app-20241213123906-0000/1 on hostPort 172.19.0.7:44865 with 10 core(s), 10.0 GiB RAM\n",
      "24/12/13 12:39:06 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241213123906-0000/2 on worker-20241213123842-172.19.0.5-40689 (172.19.0.5:40689) with 10 core(s)\n",
      "24/12/13 12:39:06 INFO StandaloneSchedulerBackend: Granted executor ID app-20241213123906-0000/2 on hostPort 172.19.0.5:40689 with 10 core(s), 10.0 GiB RAM\n",
      "24/12/13 12:39:06 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241213123906-0000/3 on worker-20241213123842-172.19.0.2-35617 (172.19.0.2:35617) with 10 core(s)\n",
      "24/12/13 12:39:06 INFO StandaloneSchedulerBackend: Granted executor ID app-20241213123906-0000/3 on hostPort 172.19.0.2:35617 with 10 core(s), 10.0 GiB RAM\n",
      "24/12/13 12:39:06 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241213123906-0000/4 on worker-20241213123842-172.19.0.6-40959 (172.19.0.6:40959) with 10 core(s)\n",
      "24/12/13 12:39:06 INFO StandaloneSchedulerBackend: Granted executor ID app-20241213123906-0000/4 on hostPort 172.19.0.6:40959 with 10 core(s), 10.0 GiB RAM\n",
      "24/12/13 12:39:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/12/13 12:39:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 6cad2e46d2a5, 40289, None)\n",
      "24/12/13 12:39:06 INFO BlockManagerMasterEndpoint: Registering block manager 6cad2e46d2a5:40289 with 15.8 GiB RAM, BlockManagerId(driver, 6cad2e46d2a5, 40289, None)\n",
      "24/12/13 12:39:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 6cad2e46d2a5, 40289, None)\n",
      "24/12/13 12:39:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 6cad2e46d2a5, 40289, None)\n",
      "24/12/13 12:39:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241213123906-0000/4 is now RUNNING\n",
      "24/12/13 12:39:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241213123906-0000/0 is now RUNNING\n",
      "24/12/13 12:39:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241213123906-0000/1 is now RUNNING\n",
      "24/12/13 12:39:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241213123906-0000/2 is now RUNNING\n",
      "24/12/13 12:39:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241213123906-0000/3 is now RUNNING\n",
      "24/12/13 12:39:06 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@ba38bfa\n",
       "\u001b[36msqlContext\u001b[39m: \u001b[32mSQLContext\u001b[39m = org.apache.spark.sql.SQLContext@482cd4e5\n",
       "\u001b[32mimport \u001b[39m\u001b[36msqlContext.implicits._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.SparkConf\n",
    "val spark = SparkSession.builder.master(\"spark://spark:7077\").config(\"spark.executor.memory\", \"10g\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "val sqlContext = spark.sqlContext\n",
    "import sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msearch_files\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search_files(path: String, extension: String): ArrayBuffer[Path] = {\n",
    "  val files = ArrayBuffer.empty[Path]\n",
    "  val root = Paths.get(path)\n",
    "\n",
    "  Files.walkFileTree(root, new SimpleFileVisitor[Path] {\n",
    "    override def visitFile(file: Path, attrs: BasicFileAttributes) = {\n",
    "      if (file.getFileName.toString.endsWith(extension)) {\n",
    "        files += file\n",
    "      }\n",
    "      FileVisitResult.CONTINUE\n",
    "    }\n",
    "  })\n",
    "\n",
    "  files\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-----------------+---+-------------------+----+-----------+-----+----------+---------------+-------------------+--------------------+--------------------+--------------------+-----+---------------------+-----------+-------+--------------------+--------------------+-----------------+------------+\n",
      "|ResoTrigger|               Time|         Position|Lap|           Velocity|Pump|optotrigger|Licks|Licking_MM|Position_binned|Position_binned_2cm|1_cm_binned_position|2_cm_binned_position|5_cm_binned_position|Lap_2|Stimulation condition|Stimulation|Zone ID|           file_name|         neural_data|__index_level_0__|Time_counter|\n",
      "+-----------+-------------------+-----------------+---+-------------------+----+-----------+-----+----------+---------------+-------------------+--------------------+--------------------+--------------------+-----+---------------------+-----------+-------+--------------------+--------------------+-----------------+------------+\n",
      "|          1|                0.0|211.1310592459605|1.0|-0.1730679442129551| 0.0|        1.0|  0.0|       0.0| [211.0, 212.0)|     [210.0, 212.0)|                 212|                 106|                  43|    1|                    0|      false|      4|ID18158/Day5/Data...|[-0.0453613288700...|                0|        NULL|\n",
      "|          2|0.03124183155095408|211.1310592459605|1.0| -0.404976346058852| 0.0|        1.0|  0.0|       0.0| [211.0, 212.0)|     [210.0, 212.0)|                 212|                 106|                  43|    1|                    0|      false|      4|ID18158/Day5/Data...|[-0.0064532286487...|                1|        NULL|\n",
      "|          3|0.06248366310190815|211.1310592459605|1.0|-0.5747038750782638| 0.0|        1.0|  0.0|       0.0| [211.0, 212.0)|     [210.0, 212.0)|                 212|                 106|                  43|    1|                    0|      false|      4|ID18158/Day5/Data...|[-0.0010424358770...|                2|        NULL|\n",
      "|          4|0.09372549465286223|211.1310592459605|1.0|-0.6913096533078872| 0.0|        1.0|  0.0|       0.0| [211.0, 212.0)|     [210.0, 212.0)|                 212|                 106|                  43|    1|                    0|      false|      4|ID18158/Day5/Data...|[-0.0379230715334...|                3|        NULL|\n",
      "|          5| 0.1249673262038163|211.1310592459605|1.0|-0.7633097032637437| 0.0|        1.0|  0.0|       0.0| [211.0, 212.0)|     [210.0, 212.0)|                 212|                 106|                  43|    1|                    0|      false|      4|ID18158/Day5/Data...|[-0.0092158457264...|                4|        NULL|\n",
      "+-----------+-------------------+-----------------+---+-------------------+----+-----------+-----+----------+---------------+-------------------+--------------------+--------------------+--------------------+-----+---------------------+-----------+-------+--------------------+--------------------+-----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_array\u001b[39m: \u001b[32mArrayBuffer\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m] = \u001b[33mArrayBuffer\u001b[39m(\n",
       "  [ResoTrigger: bigint, Time: double ... 19 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 19 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 19 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 19 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 19 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 19 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 19 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 17 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 18 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 19 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "  [ResoTrigger: bigint, Time: double ... 20 more fields],\n",
       "...\n",
       "\u001b[36mdf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [ResoTrigger: bigint, Time: double ... 20 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load parquet files into a DataFrame array\n",
    "val df_array = search_files(\"/main/data/source_as_parquet\", \".parquet\")\n",
    "  .map { f =>\n",
    "    spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(f.toString)\n",
    "  }\n",
    "// Concat all\n",
    "val df = df_array.reduce((df1, df2) => df1.unionByName(df2, allowMissingColumns=true))\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+-----------------+----+--------------------+--------------------+--------------------+-------------------+------------------+\n",
      "|              time|         position|original_velocity|pump|         neural_data|           file_name|                  dt|                 dx|          velocity|\n",
      "+------------------+-----------------+-----------------+----+--------------------+--------------------+--------------------+-------------------+------------------+\n",
      "|0.9684967780795763|167.8834395309168|15.33076223119325| 0.0|[0.00292782419273...|ID17905/Day5/Data...|0.031241831550954058|0.11094945690160785|3.5513108993195277|\n",
      "|0.9997386096305304|168.0412270034912|13.94933848866475| 0.0|[0.00290607741044...|ID17905/Day5/Data...|0.031241831550954058|0.15778747257439818| 5.050519279481222|\n",
      "| 1.030980441181484|168.2425834116714|12.65062656640945| 0.0|[0.00314377625181...|ID17905/Day5/Data...|0.031241831550953614|0.20135640818020306| 6.445089746156606|\n",
      "| 1.062222272732439|168.4732800797663|11.48072543172317| 0.0|[0.00356999124778...|ID17905/Day5/Data...|0.031241831550955057| 0.2306966680949074| 7.384223543957206|\n",
      "| 1.093464104283393| 168.707300343927|10.46035441083599| 0.0|[0.00456834512351...|ID17905/Day5/Data...|0.031241831550953947|0.23402026416067656|   7.4906064255228|\n",
      "+------------------+-----------------+-----------------+----+--------------------+--------------------+--------------------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mwindowSpec\u001b[39m: \u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@177fc108\n",
       "\u001b[36mdv\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [time: double, position: double ... 7 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val windowSpec = Window.partitionBy(\"file_name\").orderBy(\"time\")\n",
    "val dv = df.select(\n",
    "      $\"Time\".as(\"time\"),\n",
    "      $\"Position\".as(\"position\"),\n",
    "      $\"Velocity\".as(\"original_velocity\"),\n",
    "      $\"Pump\".as(\"pump\"),\n",
    "      $\"neural_data\",\n",
    "      $\"file_name\"\n",
    ")\n",
    ".withColumn(\"dt\", \n",
    "  $\"time\" - when(\n",
    "    (lag(\"time\", 1).over(windowSpec)).isNull, \n",
    "    0\n",
    "  ).otherwise(\n",
    "    lag(\"time\", 1).over(windowSpec)\n",
    "  )\n",
    ")\n",
    ".withColumn(\"dx\",\n",
    "  $\"position\" - when(\n",
    "    (lag(\"position\", 1).over(windowSpec)).isNull, \n",
    "    0\n",
    "  ).otherwise(\n",
    "    lag(\"position\", 1).over(windowSpec)\n",
    "  )\n",
    ")\n",
    ".withColumn(\"velocity\",\n",
    "  when(\n",
    "    $\"dt\" === 0, \n",
    "    0\n",
    "  ).otherwise(\n",
    "    $\"dx\" / $\"dt\"\n",
    "  )\n",
    ")\n",
    ".na.drop()\n",
    ".where(!($\"pump\" < 1 && $\"pump\" > 0))\n",
    ".where(\"exists(neural_data, x -> x is not null)\")\n",
    "dv.createOrReplaceTempView(\"dv\")\n",
    "dv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|pump|\n",
      "+----+\n",
      "| 0.0|\n",
      "| 1.0|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct(pump) from dv limit 5\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "java.io.IOException",
     "evalue": "Permission denied",
     "output_type": "error",
     "traceback": [
      "\u001b[31mjava.io.IOException: Permission denied\u001b[39m",
      "  java.io.UnixFileSystem.createFileExclusively(\u001b[32mNative Method\u001b[39m)",
      "  java.io.File.createNewFile(\u001b[32mFile.java\u001b[39m:\u001b[32m1023\u001b[39m)",
      "  ammonite.$sess.cmd7$Helper.<init>(\u001b[32mcmd7.sc\u001b[39m:\u001b[32m5\u001b[39m)",
      "  ammonite.$sess.cmd7$.<init>(\u001b[32mcmd7.sc\u001b[39m:\u001b[32m7\u001b[39m)",
      "  ammonite.$sess.cmd7$.<clinit>(\u001b[32mcmd7.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "import java.io.File\n",
    "\n",
    "val directory = new File(\"/main/data/transformed\")\n",
    "val file = new File(directory, \"test.txt\")\n",
    "file.createNewFile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": "Unable to clear output directory file:/main/data/transformed prior to writing to it.",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.SparkException: Unable to clear output directory file:/main/data/transformed prior to writing to it.\u001b[39m",
      "  org.apache.spark.sql.errors.QueryExecutionErrors$.cannotClearOutputDirectoryError(\u001b[32mQueryExecutionErrors.scala\u001b[39m:\u001b[32m795\u001b[39m)",
      "  org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(\u001b[32mInsertIntoHadoopFsRelationCommand.scala\u001b[39m:\u001b[32m239\u001b[39m)",
      "  org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(\u001b[32mInsertIntoHadoopFsRelationCommand.scala\u001b[39m:\u001b[32m131\u001b[39m)",
      "  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m113\u001b[39m)",
      "  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(\u001b[32mcommands.scala\u001b[39m:\u001b[32m111\u001b[39m)",
      "  org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(\u001b[32mcommands.scala\u001b[39m:\u001b[32m125\u001b[39m)",
      "  org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(\u001b[32mAdaptiveSparkPlanExec.scala\u001b[39m:\u001b[32m390\u001b[39m)",
      "  org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(\u001b[32mAdaptiveSparkPlanExec.scala\u001b[39m:\u001b[32m418\u001b[39m)",
      "  org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(\u001b[32mAdaptiveSparkPlanExec.scala\u001b[39m:\u001b[32m390\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m107\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m125\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m201\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m108\u001b[39m)",
      "  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m900\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m66\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m107\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m98\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m461\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32morigin.scala\u001b[39m:\u001b[32m76\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m461\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m32\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m267\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m263\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m32\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m32\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m437\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m98\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m85\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.commandExecuted(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m83\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m142\u001b[39m)",
      "  org.apache.spark.sql.DataFrameWriter.runCommand(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m859\u001b[39m)",
      "  org.apache.spark.sql.DataFrameWriter.saveToV1Source(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m388\u001b[39m)",
      "  org.apache.spark.sql.DataFrameWriter.saveInternal(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m361\u001b[39m)",
      "  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m240\u001b[39m)",
      "  org.apache.spark.sql.DataFrameWriter.parquet(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m792\u001b[39m)",
      "  ammonite.$sess.cmd9$Helper.<init>(\u001b[32mcmd9.sc\u001b[39m:\u001b[32m1\u001b[39m)",
      "  ammonite.$sess.cmd9$.<init>(\u001b[32mcmd9.sc\u001b[39m:\u001b[32m7\u001b[39m)",
      "  ammonite.$sess.cmd9$.<clinit>(\u001b[32mcmd9.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "dv.write.partitionBy(\"file_name\").mode(\"overwrite\").parquet(\"/main/data/transformed/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
